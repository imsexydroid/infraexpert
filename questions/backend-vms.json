{
    "uid": "backend-vms",
    "name": "Backend VMs",
    "acl": {
        "isFree": true,
        "isFreeForStudents": false,
        "productRequired": [
            "infraexpert"
        ],
        "isAvailable": true
    },
    "releaseDate": "",
    "isReleased": true,
    "video": {
        "vimeoId": "800271574",
        "duration": 0,
        "annotations": [],
        "instructor": "",
        "style": null
    },
    "prompt": "<p>You have several VMs set up behind a load balancer with auto-scaling on.\n    Explain how this architecture would work and why we might choose this\n    implementation.</p>\n\n<h2>Follow-Up Questions</h2>\n<ol>\n    <li>You notice that your application back-end VMs are running at very low\n    utilization. What can you do?</li>\n    <li>Explain what tools or troubleshooting steps you would use to find the root\n    cause.</li>\n    <li>What are some ways you can optimize utilization or ensure this doesn't happen\n    in the future?</li>\n    <li>The VMs behind your load balancer are receiving uneven distribution of\n    traffic. Why might this be?</li>\n    <li>The VMs behind your load balancer aren't responding to traffic on the\n    configured data port. Why might this be and how would you troubleshoot this?</li>\n</ol>",
    "walkthrough": [
        {
            "title": "How this architecture would work and why we might choose this implementation.",
            "content": "<p>\n    As with many infrastructure interview questions, we would want to probe\n    the interviewer for more information, gather system requirements and better\n    understand the situation to figure out the best solution. Understanding more\n    about the VMs and loadbalancers (ex. how many load balancers, what algorithm\n    they use, what VMs we are running, etc.) would help decide on and justify\n    the\n    architecture. In any case, this set up with the Load\n    Balancing helps to manage incoming requests by optimally routing traffic so\n    that no one instance or VM is overwhelmed. This reduces latency & ensures\n    high availability.\n<p>\n    Assuming this is implemented in the cloud, we can explain the\n    architecture\n    based on a cloud provider implementation we are familiar with. For\n    example,\n    in AWS, you can set up Elastic Load Balancing by attaching the load\n    balancer to your Auto Scaling group. This registers the group with the\n    load\n    balancer, which acts as a single point of contact for all incoming web\n    traffic to your Auto Scaling group, which automatically distributes your\n    incoming application\n    traffic across all the EC2 instances that you are running. Instances\n    that are launched by your Auto Scaling group are automatically\n    registered with the load balancer. Likewise, instances that are\n    terminated\n    by your Auto Scaling group are automatically deregistered from the load\n    balancer. It typically works this way in all large cloud providers so\n    this explanation should suffice, but if you can get into specifics of a\n    Cloud Provider that's even better.\n</p>\n<p>\n    Additionally, the architecture would have implemented\n    monitoring and alerting on.\n    The alerts would be set on various load parameters of the service.\n    On receiving an alert on CPU/ Memory/ IO from the alerting mechanism\n    the containers hosting the executable would be multiplied with a\n    pre-configured multiplication factor. Furthermore, there should be a\n    mechanism\n    to scale the underlying host machines to accommodate the growing number\n    of\n    VMs.\n</p>\n<p>\n    In most cloud providers, you can\n    configure your Auto Scaling group to use Elastic Load Balancing metrics\n    (such as the Application Load Balancer request count per target) to\n    scale\n    the number of instances in the group as demand fluctuates.\n\n    Optionally, you can add health checks to your\n    Auto\n    Scaling group so that\n    unhealthy instances can be identified and replcaed based on these\n    additional health checks. Otherwise,\n    you\n    can create an alarm (called Cloudwatch in AWS) that notifies you if the\n    healthy host\n    count of\n    the target group is lower than allowed.\n</p>\n\n\n</p>"
        },
        {
            "title": "You notice that your application back-end VMs are running at very low utilization. What can you do?",
            "content": "<p>\nConfigure a scale-in mechanism by integrating scale-in triggers on\nutilization metrics.When configuring a scale-in mechanism, you are defining the specific triggers that will cause the number of VMs to be reduced. For example, you might configure a scale-in mechanism that reduces the number of VMs when the average CPU utilization across all VMs falls below a certain threshold for a certain amount of time. The trigger is the low CPU utilization, and the action is to scale in, or reduce the number of VMs.\n\nBy integrating scale-in triggers based on utilization metrics, you can ensure that your system only runs the minimum number of VMs necessary to meet demand, reducing waste and maximizing resource utilization.\n\nWays to Address Low Utilization of Application Back-End VMs:\n    <ol>\n      <li>\n        <p>Right-sizing: Ensure that the VMs are appropriately sized for the workloads they are running. VMs that are over-provisioned may result in low utilization.</p>\n      </li>\n      <li>\n        <p>Load balancing: If some VMs are heavily utilized while others are idle, you can distribute the workload more evenly across all VMs by implementing load balancing techniques.</p>\n      </li>\n      <li>\n        <p>Automated scaling: Set up auto-scaling policies that dynamically add or remove VMs based on changes in workload demand.</p>\n      </li>\n      <li>\n        <p>Configuring a Scale-In Mechanism: setting up triggers that automatically reduce the number of VMs running in your application back-end in response to changes in utilization metrics.</p>\n      </li>\n      <li>\n        <p>Resource optimization: Make sure that resources such as CPU, memory, and storage are being used efficiently and that there are no bottlenecks.</p>\n      </li>\n      <li>\n        <p>Software optimization: Optimize the software and applications running on the VMs for performance and resource utilization.</p>\n      </li>\n      <li>\n        <p>Batch processing: If the workload includes batch processing jobs, consider running them on dedicated VMs during off-peak hours to maximize utilization.</p>\n      </li>\n    </ol>\n</p>\n<h2>Troubleshooting root cause:</h2>\n<ol>\n  <li>\n    <h3>Step 1: Use Monitoring Tools</h3>\n    <p>I would start by using performance monitoring tools to track utilization metrics like CPU, memory, and network usage. This would give me a good idea of which resources are underutilized and where the bottlenecks are.</p>\n  </li>\n  <li>\n    <h3>Step 2: Review Logs and Error Messages</h3>\n    <p>Next, I would review logs and error messages to see if anything is affecting performance. I would look for errors or warnings related to the application, the operating system, or the infrastructure.</p>\n  </li>\n  <li>\n    <h3>Step 3: Analyze Resource Distribution</h3>\n    <p>I would then analyze the distribution of resources like CPU and memory to make sure the VMs are configured appropriately. This would help me spot any resource constraints that might be causing low utilization.</p>\n  </li>\n  <li>\n    <h3>Step 4: Check the Network Infrastructure</h3>\n    <p>I would also check the network infrastructure for any issues that could be affecting performance, such as bottlenecks or configuration errors.</p>\n  </li>\n  <li>\n    <h3>Step 5: Review the Application Configuration</h3>\n    <p>Finally, I would review the configuration and settings of the application and its dependencies. I would make sure they are optimized for performance and resource utilization.</p>\n  </li>\n</ol>\n<p>By following these steps, I would be able to identify the root cause of low utilization in the back-end VMs and take the necessary steps to resolve the issue and optimize performance!</p>\n\n<h2>Optimizing utilization and avoiding future issues:</h2>\n<ul>\n  <li>Implement autoscaling to dynamically adjust the number of VMs based on utilization metrics.</li>\n  <li>Monitor resource utilization and performance regularly and proactively address any issues that arise.</li>\n  <li>Optimize the application and its dependencies for performance and resource utilization, including tuning parameters and configuration settings.</li>\n  <li>Consider using load balancing and distribution algorithms to distribute load evenly across the VMs.</li>\n  <li>Perform regular maintenance and upgrades to the infrastructure to ensure optimal performance and resource utilization.</li>\n</ul>"
        },
        {
            "title": "What are some ways you can optimize utilization or ensure this doesn't happen in the future?",
            "content": "<p>\n    To optimize utilization, automation is required based on policies. Usually\n    there\u2019s policies set for scale-in/scale-out of processing units. There\u2019s\n    also policies for archiving/deleting data. To ensure that similar incidents\n    don\u2019t happen in the future we could also introduce a new SLI or modify an existing SLI\n    and update the corresponding SLO. We would also want to keep a playbook to track and\n    document such incidents in case they re-occur.\n</p>"
        },
        {
            "title": "The VMs behind your load balancer are receiving uneven distribution of traffic. Why might this be?",
            "content": "<ul>\n    <li>\n        <b>Cause 1: </b> The load balancer might be configured in sticky-session\n        or persistence mode. It\u2019s advisable\n        to use stateless web servers with decoupled distributed session caching\n        to avoid this. In a cloud set up, you can fix this by updating session\n        persistence to be None so traffic is distributed across all healthy\n        instances in the backend pool.\n    </li>\n    <li>\n        <b>Cause 2: </b> This might be caused by a proxy. Clients that run\n        behind proxies might be seen as one unique client application from the\n        load balancer's point of view. This can be fixed by removing or\n        reconfiguring the proxy.\n    </li>\n</ul>"
        },
        {
            "title": "The VMs behind your load balancer aren't responding to traffic on the configured data port. Why might this be and how would you troubleshoot this?",
            "content": "<p>\n    If a backend pool VM is listed as healthy and responds to the health probes,\n    but is still not participating in the load balancing, or isn't responding to\n    the data traffic, it might be because:\n    The VMs that are not responding from behind a load balancer are either down\n    or service that are supposed to be running to respond on the data port is\n    down or it\u2019s misconfigured for a different port.\n</p>\n<ul>\n    <li>\n        <b>Cause 1: </b> A load balancer backend pool VM isn't listening on the\n        data port. You can troubleshoot this scenario by opening a prompt on the\n        VM and running netstat -an command to validate there's an application\n        listening on the data port. If the port is marked as <b>LISTENING </b>,\n        then we need to check the target app on that port for any possible\n        issues.isn't listed with state <b>LISTENING</b>, configure the proper\n        listener port\n    </li>\n    <li>\n        <b>Cause 2: </b> A Network security group is blocking the port on the\n        load balancer backend pool VM. To troubleshoot this, we must check the\n        network security groups of the VM and check From the list of network\n        security groups, check if the incoming or outgoing traffic on the data\n        port has interference. If any of the rules are blocking the traffic,\n        remove and reconfigure those rules to allow the data traffic.\n    </li>\n</ul>"
        }
    ],
    "hints": []
}